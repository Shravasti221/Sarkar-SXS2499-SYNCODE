# Multi-Agent Orchestrated Workflow with LangGraph

This repository implements a **dynamic, multi-agent orchestration framework** built on [LangGraph](https://github.com/langchain-ai/langgraph).
It enables **conversational coordination between multiple expert LLM agents**, a central orchestrator, user simulators, and shared API tools — all working in a cyclic, reasoning-based workflow.

---
# Pending Items:
Here’s a clean way to structure your **to-do list** for this project. I’ve also added one more logical item based on your workflow gaps:

---

### To-Do List

* [ ] Pass API IDs for all tools so that the **API Execution Node** can corroborate parameters and validate whether clarifications are required.
* [ ] Ensure **LLMs don’t go in circles** by checking if questions or actions are disjoint from the existing dialogue history.
* [ ] Implement **loop detection / cycle prevention** in the workflow graph, especially for conditional edges (expert → API → expert → orchestrator).
* [x] Add **validation for expert JSON responses**, e.g., verify required keys (`route`, `task.name`, `task.params`) and fallback for malformed outputs.
* [ ] Standardize **API result format** to ensure all agents interpret results consistently.
* [ ] Add **logging / persistence** of full `EventState` for post-run analysis and debugging.
* [ ] Implement **step-through mode** for debugging chat flow and agent decisions (pause after each node, print last message).
* [ ] Consider **rate-limiting or throttling** LLM/API calls to prevent infinite loops or repeated invocations.
* [ ]  **Error handling for API failures**: Implement retries, fallback responses, or escalation if an API consistently fails.
* [ ] **Conversation summarization**: Add a mechanism to summarize long chats for easier review or reporting.

## Overview

The system models a **conversation-driven event management environment**, where:

* **ProblemCreator** generates a scenario.
* **User** acts within that scenario.
* **Orchestrator (Junior Assistant)** interprets the user’s intent and dynamically routes control to:

  * An **Expert Agent** (domain-specific specialist),
  * The **API Execution Node** (for tool usage),
  * Or back to the **User**.

Each **Expert** has:

* A custom **system prompt** (`expert_prompt` in `experts.json`)
* Internal logic to decide whether to:

  * Continue reasoning (return to orchestrator), or
  * Trigger an API execution (via structured JSON routing).

---

## System Architecture

### Core Components

| Component           | File               | Role                                                                                                                                  |
| ------------------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------- |
| **Expert**          | `expert.py`        | Defines specialized domain agents. Each expert has its own prompt, available tools, and routing logic (`route`, `evaluate_api_exec`). |
| **Orchestrator**    | `orchestrator.py`  | Central router (junior assistant) that parses JSON outputs from LLMs and directs flow to appropriate nodes.                           |
| **APIPipeline**     | `api_execution.py` | Executes API tasks generated by experts. Returns results to the correct expert or orchestrator.                                       |
| **Problem Creator** | `simulators.py`    | Generates a starting event scenario using the LLM.                                                                                    |
| **User Simulator**  | `simulators.py`    | Simulates realistic user responses based on context and chat history.                                                                 |
| **Helpers**         | `helpers.py`       | Defines `EventState`, the shared memory object containing conversation history, tasks, and routing metadata.                          |

---

## Data Flow

```
START
  ↓
Problem Creator
  ↓
User Simulator
  ↕
Orchestrator
   ├<-→ (ExpertNode <-> APIExecution)*  (conditional loops)
   └-→ END
```

### Routing Logic

* **Orchestrator:** Uses JSON responses of the form
  `{"route": "user" | "api_execution" | "expert_name" | "end", "task": {...}}`
* **Experts:** Decide based on API responses and chat context whether to:

  * Continue reasoning (`"route": "orchestrator"`)
  * Trigger an API call (`"route": "api_execution"`)
* **APIExecution:** Returns data and routes back to the correct caller using `state["caller"]`.

---

## State Definition

Defined in `helpers.py`:

```python
class EventState(TypedDict):
    user_request: str
    problem_created: Dict
    problem_statement: str
    chat_history: List[Dict[str, str]]
    api_task: Optional[ApiCall]
    api_result: Dict
    caller: str
```

---

## Example `experts.json`

```json
{
  "VenueExpert": {
    "description": "Handles venue booking and logistics.",
    "expert_prompt": "You are an expert in venue management and logistics.",
    "apis": {
      "book_venue": {
        "params": {"date": "str", "guests": "int", "location": "str"}
      }
    }
  },
  "CateringExpert": {
    "description": "Manages food and beverages for events.",
    "expert_prompt": "You specialize in catering arrangements for large events.",
    "apis": {
      "arrange_catering": {
        "params": {"guests": "int", "menu_type": "str"}
      }
    }
  }
}
```

---

## Running the Workflow

### 1. Install Dependencies

```bash
pip install langgraph langchain-openai IPython
```

### 2. Set Environment Variable

```bash
export GROQ_API_KEY="your_groq_api_key"
```

### 3. Run the System

```bash
python main.py
```
---

## Example Output (Simplified)

```
[ProblemCreatorLLM] You are organizing an event for 100 guests on July 15th.
[User] I need help booking a venue.
[JuniorAssistant] Routing to VenueExpert...
[VenueExpert] {"route": "api_execution", "task": {"name": "book_venue", "params": {...}}}
[APIExecutionNode] Venue booked successfully.
[VenueExpert-EvalAPI] {"route": "orchestrator"}
[JuniorAssistant] Task completed. Anything else?
```

---

## Key Design Principles

* **Declarative Routing:** All agents emit structured JSON to control flow.
* **LLM Autonomy:** Experts and orchestrator reason and decide routing themselves.
* **Transparency:** Conversation history is logged in `EventState.chat_history`.
* **Extensibility:** Add new experts by simply updating `experts.json`.

---

## Customization

To add a new expert:

1. Define it in `experts.json` with its description, prompt, and API schema.
2. The main script will automatically add its node and routing edges.

---

## Future Extensions

* Integrate real REST APIs instead of mock calls.
* Add memory persistence via a database or vector store.
* Implement user feedback loops for reinforcement.

---



## Project Pitch:
## **Dataset Generation (Multi Agent+ Tool Use) Idea**
*Currently Multi-Agent Multi Turn but can simply be configured to have 1 agent.*

Based on my review of relevant research papers, I identified a gap in datasets for training multi-agent tool-use systems. To address this, I propose methods for dataset creation, outlined at the end of this document. As a starting point, I decided to develop a simpler proof-of-concept system to validate the approach before scaling to more complex implementations.

**System Overview**  
The system simulates a user, modeled as a junior field assistant, interacting with an orchestrator LLM connected to specialized expert LLMs within an event planning company. These experts include a Logistics Expert, Venue Manager, and Catering Manager, each handling distinct units. To simulate problem creation, a Problem Creator module generates tasks that serve as hints for the user-simulating LLM. 

Eg Problem: *A local community fundraiser is scheduled for next Sunday at 7 PM in the town hall, expecting about 120 guests. During the final rehearsal, the volunteer coordinator discovered that the “Garden Cleanup” and “Dinner Prep” breakout rooms were accidentally assigned to the same time slot. This double‑booking will force volunteers into two rooms simultaneously, causing confusion about where to report and potentially delaying the start of the main event. The issue is minor, but it needs quick resolution to keep the schedule on track*. 

The User LLM continuously validates whether the current solution (based on reviewing the entire chat history) sufficiently addresses the task. The user is restricted to asking simple, first-person questions to maintain consistency and simplicity in interactions.
```
Problem Injection (Or generation)
   ↓
 User ↴
  └─ Orchestrator LLM
     ↕── Expert 1: Logistics Expert <--------------------------←
     |    ↳ LLM generates JSON output           ↑              ↑ 
     |    ↳ Refine Output to parseable JSON     |              |
     |    ↳ Route Selector                      |              |
     |        ├── response → [orchestrator]     |              |
     |        ├── api_execution →--> clarification ------->api_execution
     |                              [arg validator]    [LLM backed Mock Server]
     ├──→ Expert 2: Venue Manager (similar flow)--------------→↑
     ├──→ Expert 3: Catering Manager (similar flow)-----------→|
```
- **Orchestrator LLM**: Acts as the central coordinator, routing user queries to the appropriate expert LLM based on the task.
- **Expert LLMs**: Each specializes in a domain (logistics, venue, catering) and generates structured JSON outputs.
- **Refine Chain**: Ensures the JSON output is correctly formatted and consistent.
- **Route Selector**: Determines the next step—returning a response to the orchestrator, or executing an API call.
- **API Execution Chain**: Includes a clarification/argument validator to ensure API calls are valid before execution by an API-specific LLM.

**Next steps I was hoping to discuss**  
Based on the described architecture, I was hoping to get suggestions to improve the system prompts for the LLMs amd the overall architecture change ideas, from research papers since we have reviewed different sets of papers.

**1. Sharper LLM Prompts**
I want to improve the system prompts for my Orchestrator, Expert, and User-Simulating LLMs using ideas from multi-agent research. Any suggestions for making role definitions clearer and outputs more consistent?

**2. Architecture Upgrades**
Exploring ways to strengthen the setup—like adding a “committee” of LLMs to check conversation quality (coherence, relevance, task fit). What other design tweaks could make the system smoother or smarter?

**3. Reducing LLM Uncertainty**
I’d like to replace some LLM-based steps with rule-based or algorithmic methods to make the pipeline faster and more reliable. Which parts (e.g., routing, validation) would benefit most from this?

**4. Direct Expert–User Interaction**
Would it help if Expert LLMs handled simpler user queries directly, skipping the Orchestrator? How could I test whether this performs better than my current setup?

**5. Turning Queries into Conversations**
I simply use a problem creator llm to generate single queries/problem scenarios (like from DevRev) and want to turn them into multi-turn dialogues. Feeding them as problem statements to the user simulator (as hints) works, but what other approaches could generate natural, multi-step interactions?

**6. Comparing Architectures**
Some ways to quickly compare different designs—like Orchestrator vs. direct Expert–User flow? I’m looking for practical metrics or lightweight tests.


 **Why I Chose This Approach**

* Open-source APIs tend to be highly varied in functionality, making it difficult to maintain domain consistency.
* Generating a curated list of APIs within a single domain provides a clearer and more accurate representation of the problem statement we aim to address.
* The dataset generation architecture is reusable, since most of its components are configurable.
* This configurable design allows the system to be easily adapted for multiple domains or use cases with minimal changes. I was hoping that even for the different user types shared by DevRev, we would simply be able to change the sort of experts we are working with and convert the queries into a conversation. In case only a single expert is required: i.e. single agent environment, it is still configurable.

One Issue I do forsee is that this is becoming in some parts more of an engineering/dev problem rather than a research problem so I was hoping to get everyone's opinions on it on what could be improved or what changes/ exploring could be done based on the papers we have read.

**Author:** Shravasti Sarkar
**Version:** 0.1
**Framework:** LangGraph + Groq LLM Integration
